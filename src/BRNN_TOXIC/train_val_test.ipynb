{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "STOPS = stopwords.words('english')\n",
    "PROJ_NAME = \"BRNN_TOXIC\"\n",
    "MAX_COMMENT_LENGTH = 1500\n",
    "from collections import Counter\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import word2vec\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "STOPS = stopwords.words('english')\n",
    "PROJ_NAME = \"BRNN_TOXIC\"\n",
    "MAX_COMMENT_LENGTH = 1500\n",
    "from collections\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\",'r') as f:\n",
    "    config_file = json.load(f)[\"BASE_CONFIG\"]\n",
    "with open(config_file,'r') as f:\n",
    "    config = json.load(f)\n",
    "data_dir=os.path.join(config[\"data_dir\"],PROJ_NAME)\n",
    "model_dir=os.path.join(config[\"model_dir\"],PROJ_NAME)\n",
    "out_dir=os.path.join(config[\"out_dir\"],PROJ_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load(os.path.join(data_dir,\"text.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41913, (41913, 100))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab),model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0626680286518227\n",
      "0.1253360573036454\n",
      "0.1880040859554681\n",
      "0.2506721146072908\n",
      "0.3133401432591135\n",
      "0.3760081719109362\n",
      "0.4386762005627589\n",
      "0.5013442292145815\n",
      "0.5640122578664043\n",
      "0.626680286518227\n",
      "0.6893483151700497\n",
      "0.7520163438218724\n",
      "0.8146843724736951\n",
      "0.8773524011255178\n",
      "0.9400204297773405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6789972"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find 50 most often negative and 50 most often positive words\n",
    "df = pd.read_csv(os.path.join(data_dir,\"train.csv\"))\n",
    "dfs = {}\n",
    "null = df.copy()\n",
    "for name in LABELS:\n",
    "    dfs[name] = df.loc[(df[name] == 1)]\n",
    "    null.drop(null[null[name]==1].index,axis=0,inplace=True)\n",
    "        \n",
    "\n",
    "total_words = 0\n",
    "all_words = set()\n",
    "i = 0\n",
    "for comment in df[\"comment_text\"]:\n",
    "    if i%10000 == 0:\n",
    "        print(i/len(df))\n",
    "    words = word_tokenize(comment)\n",
    "    if len(words) < MAX_COMMENT_LENGTH:\n",
    "        for word in words:\n",
    "            if word not in STOPS and word not in string.punctuation:\n",
    "                total_words+=1\n",
    "                all_words.add(word.lower())\n",
    "    i+=1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "frequency_dict = {}\n",
    "i=0\n",
    "for c,d in dfs.items():\n",
    "    print(i)\n",
    "    word_freq = {}\n",
    "    for class_comment in d[\"comment_text\"]:\n",
    "        for word in word_tokenize(class_comment):\n",
    "            if word in all_words and word not in string.punctuation:\n",
    "                if word in word_freq.keys():\n",
    "                    word_freq[word] = word_freq[word]+1\n",
    "                else:\n",
    "                    word_freq[word]=1\n",
    "    frequency_dict[c] = word_freq\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_freq = {}\n",
    "for class_comment in null[\"comment_text\"]:\n",
    "        for word in word_tokenize(class_comment):\n",
    "            if word in all_words and word not in string.punctuation:\n",
    "                if word in null_freq.keys():\n",
    "                    null_freq[word] = null_freq[word]+1\n",
    "                else:\n",
    "                    null_freq[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'a',\n",
       " 'the',\n",
       " 'to',\n",
       " 'and',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'of',\n",
       " 'your',\n",
       " 'that',\n",
       " '``',\n",
       " 'it',\n",
       " 'are',\n",
       " 'fuck',\n",
       " 'ass',\n",
       " 'faggot',\n",
       " 'yourself',\n",
       " \"'re\",\n",
       " 'u',\n",
       " 'fucksex',\n",
       " 'yourselfgo',\n",
       " 'i',\n",
       " 'shit',\n",
       " \"n't\",\n",
       " 'my',\n",
       " 'fucking',\n",
       " 'in',\n",
       " 'do',\n",
       " 'on',\n",
       " 'for',\n",
       " 'me',\n",
       " \"'s\",\n",
       " 'have',\n",
       " '...',\n",
       " 'this',\n",
       " 'not',\n",
       " 'with',\n",
       " 'be',\n",
       " 'like',\n",
       " 'going',\n",
       " 'am',\n",
       " 'kill',\n",
       " 'will',\n",
       " 'murder',\n",
       " 'die',\n",
       " \"'m\",\n",
       " 'if',\n",
       " 'get',\n",
       " 'so',\n",
       " 'all',\n",
       " 'hope',\n",
       " 'go',\n",
       " 'moron',\n",
       " 'an',\n",
       " 'hi',\n",
       " 'know',\n",
       " 'as',\n",
       " 'off',\n",
       " 'what',\n",
       " 'who',\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " 'just',\n",
       " 'up',\n",
       " 'about',\n",
       " 'gay',\n",
       " 'nigger',\n",
       " 'licker',\n",
       " 'ca',\n",
       " 'down',\n",
       " 'or',\n",
       " 'keep',\n",
       " 'people',\n",
       " 'nigga',\n",
       " 'they',\n",
       " 'but',\n",
       " 'by',\n",
       " 'from',\n",
       " 'article',\n",
       " 'page',\n",
       " 'at',\n",
       " 'can',\n",
       " 'would',\n",
       " 'there',\n",
       " 'talk',\n",
       " 'one',\n",
       " 'which',\n",
       " 'been',\n",
       " 'should',\n",
       " 'any',\n",
       " 'more',\n",
       " 'no',\n",
       " 'he',\n",
       " 'other',\n",
       " 'some',\n",
       " 'we',\n",
       " 'think',\n",
       " 'here',\n",
       " 'see',\n",
       " 'his']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = []\n",
    "for k,v in frequency_dict.items():\n",
    "    print(k)\n",
    "    i = 0\n",
    "    while(i<13):\n",
    "        c = Counter(v).most_common(1)\n",
    "        if c[0][0] not in selected and c[0][0] in model.vocab:\n",
    "            selected.append(c[0][0])\n",
    "            i+=1   \n",
    "        v[c[0][0]]=0\n",
    "i=len(selected)\n",
    "while(len(selected)<100):\n",
    "    c = Counter(null_freq).most_common(1)\n",
    "    if c[0][0] not in selected and c[0][0] in model.vocab:\n",
    "        selected.append(c[0][0])\n",
    "        i+=1\n",
    "    null_freq[c[0][0]]=0\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL: 89.83211235124176\n",
      "Initial percentage of DF for toxic is 9.584448302009765\n",
      "Initial percentage of DF for severe_toxic is 0.9995550569965721\n",
      "Initial percentage of DF for obscene is 5.2948217407925\n",
      "Initial percentage of DF for threat is 0.2995531769557125\n",
      "Initial percentage of DF for insult is 4.936360616904074\n",
      "Initial percentage of DF for identity_hate is 0.8804858025581089\n",
      "Each label will now have at least 14.285714285714285 % of the origional df size\n",
      "toxic upsampled 3 times\n",
      "severe_toxic upsampled 16 times\n",
      "obscene upsampled 4 times\n",
      "threat upsampled 49 times\n",
      "insult upsampled 4 times\n",
      "identity_hate upsampled 18 times\n"
     ]
    }
   ],
   "source": [
    "def adjust_class_balance(df: pd.DataFrame, interested_labels, thresh):\n",
    "    dfs = {}\n",
    "    null = df.copy()\n",
    "    for name in interested_labels:\n",
    "        dfs[name] = df.loc[(df[name] == 1)]\n",
    "        null.drop(null[null[name]==1].index,axis=0,inplace=True)\n",
    "        \n",
    "    print(\"NULL:\", 100*(len(null)/len(df)))\n",
    "    for name, d in dfs.items():\n",
    "        print(\"Initial percentage of DF for\", name, \"is\", 100*(len(d)/len(df)))\n",
    "    \n",
    "    print(\"Each label will now have at least\", thresh*100,\"% of the origional df size\")\n",
    "    adjusted_df = null.sample(int(thresh*len(df))) # get a subsample of null cases\n",
    "    \n",
    "\n",
    "    for n, d in dfs.items():\n",
    "        i=0\n",
    "        for times in range(math.ceil((thresh/(len(d)/len(df))+1))):\n",
    "            adjusted_df = adjusted_df.append(d)\n",
    "            i+=1\n",
    "        print(n,\"upsampled\",i,\"times\")\n",
    "    return adjusted_df\n",
    "a_df = adjust_class_balance(df, LABELS, 1/(len(LABELS)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14757682, -0.04240734,  0.19148824,  0.00098323,  0.0573453 ,\n",
       "       -0.05768183, -0.15212771, -0.07680046,  0.03413858,  0.17289601,\n",
       "       -0.17418103, -0.00899368,  0.16075474, -0.02236715,  0.0478254 ,\n",
       "       -0.09537709,  0.12551017,  0.13849607,  0.08788317, -0.21410128,\n",
       "       -0.00878321, -0.04462972, -0.01825777, -0.13360611,  0.06181644,\n",
       "       -0.01524204,  0.0692144 , -0.06240428,  0.09520414, -0.0216194 ,\n",
       "       -0.1535608 ,  0.02832055, -0.22033127,  0.08884833,  0.12681127,\n",
       "       -0.13558899,  0.04884369,  0.06884765, -0.03505035,  0.00168696,\n",
       "        0.08216715,  0.05765036, -0.1474108 , -0.08059205,  0.05477362,\n",
       "       -0.12545238,  0.10483534,  0.05306536, -0.05766199,  0.0028046 ,\n",
       "       -0.00083108,  0.14653622, -0.15248394, -0.09605292,  0.1178815 ,\n",
       "       -0.14056268,  0.04231599, -0.14468089, -0.05952892, -0.03034033,\n",
       "       -0.05168787, -0.00610198,  0.10717586,  0.07031706, -0.03640174,\n",
       "       -0.08431084,  0.01758727, -0.00465056, -0.01382627, -0.02016944,\n",
       "        0.01935275, -0.01803247,  0.12817402, -0.16284722, -0.02605505,\n",
       "       -0.14463264,  0.1695707 , -0.11080392, -0.08280649, -0.22299463,\n",
       "        0.10041764, -0.006406  ,  0.02240508,  0.13806929,  0.12445614,\n",
       "       -0.06152638,  0.06573503,  0.12557602, -0.00334078, -0.11815333,\n",
       "        0.05027869,  0.03434117,  0.01941716,  0.09251081,  0.09951346,\n",
       "       -0.15023756,  0.02134777,  0.09834982, -0.17668375, -0.02015036])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.6266802865182269\n",
      "1.2533605730364539\n",
      "1.8800408595546811\n",
      "2.5067211460729077\n",
      "3.1334014325911346\n",
      "3.7600817191093623\n",
      "4.386762005627589\n",
      "5.0134422921458155\n",
      "5.640122578664044\n",
      "6.266802865182269\n",
      "6.893483151700497\n",
      "7.520163438218725\n",
      "8.14684372473695\n",
      "8.773524011255178\n",
      "9.400204297773405\n",
      "10.026884584291631\n",
      "10.65356487080986\n",
      "11.280245157328087\n",
      "11.906925443846312\n",
      "12.533605730364538\n",
      "13.160286016882766\n",
      "13.786966303400995\n",
      "14.413646589919221\n",
      "15.04032687643745\n",
      "15.667007162955676\n",
      "16.2936874494739\n",
      "16.92036773599213\n",
      "17.547048022510356\n",
      "18.173728309028583\n",
      "18.80040859554681\n",
      "19.427088882065036\n",
      "20.053769168583262\n",
      "20.680449455101492\n",
      "21.30712974161972\n",
      "21.933810028137945\n",
      "22.560490314656175\n",
      "23.187170601174397\n",
      "23.813850887692624\n",
      "24.440531174210854\n",
      "25.067211460729077\n",
      "25.693891747247307\n",
      "26.320572033765533\n",
      "26.94725232028376\n",
      "27.57393260680199\n",
      "28.200612893320216\n",
      "28.827293179838442\n",
      "29.45397346635667\n",
      "30.0806537528749\n",
      "30.707334039393125\n",
      "31.33401432591135\n",
      "31.960694612429574\n",
      "32.5873748989478\n",
      "33.21405518546603\n",
      "33.84073547198426\n",
      "34.46741575850248\n",
      "35.09409604502071\n",
      "35.72077633153894\n",
      "36.347456618057166\n",
      "36.974136904575396\n",
      "37.60081719109362\n",
      "38.22749747761185\n",
      "38.85417776413007\n",
      "39.4808580506483\n",
      "40.107538337166524\n",
      "40.734218623684754\n",
      "41.360898910202984\n",
      "41.98757919672121\n",
      "42.61425948323944\n",
      "43.24093976975767\n",
      "43.86762005627589\n",
      "44.49430034279412\n",
      "45.12098062931235\n",
      "45.747660915830565\n",
      "46.374341202348795\n",
      "47.001021488867025\n",
      "47.62770177538525\n",
      "48.25438206190348\n",
      "48.88106234842171\n",
      "49.50774263493993\n",
      "50.13442292145815\n",
      "50.76110320797639\n",
      "51.38778349449461\n",
      "52.01446378101284\n",
      "52.641144067531066\n",
      "53.267824354049296\n",
      "53.89450464056752\n",
      "54.521184927085756\n",
      "55.14786521360398\n",
      "55.77454550012221\n",
      "56.40122578664043\n",
      "57.027906073158654\n",
      "57.654586359676884\n",
      "58.28126664619511\n",
      "58.90794693271334\n",
      "59.53462721923156\n",
      "60.1613075057498\n",
      "60.78798779226802\n",
      "61.41466807878625\n",
      "62.04134836530447\n",
      "62.6680286518227\n",
      "63.294708938340925\n",
      "63.92138922485915\n",
      "64.54806951137738\n",
      "65.1747497978956\n",
      "65.80143008441384\n",
      "66.42811037093206\n",
      "67.0547906574503\n",
      "67.68147094396852\n",
      "68.30815123048674\n",
      "68.93483151700497\n",
      "69.5615118035232\n",
      "70.18819209004143\n",
      "70.81487237655965\n",
      "71.44155266307789\n",
      "72.0682329495961\n",
      "72.69491323611433\n",
      "73.32159352263255\n",
      "73.94827380915079\n",
      "74.57495409566901\n",
      "75.20163438218724\n",
      "75.82831466870546\n",
      "76.4549949552237\n",
      "77.08167524174192\n",
      "77.70835552826014\n",
      "78.33503581477838\n",
      "78.9617161012966\n",
      "79.58839638781483\n",
      "80.21507667433305\n",
      "80.84175696085128\n",
      "81.46843724736951\n",
      "82.09511753388774\n",
      "82.72179782040597\n",
      "83.34847810692419\n",
      "83.97515839344241\n",
      "84.60183867996064\n",
      "85.22851896647887\n",
      "85.8551992529971\n",
      "86.48187953951533\n",
      "87.10855982603356\n",
      "87.73524011255178\n",
      "88.36192039907\n",
      "88.98860068558824\n",
      "89.61528097210646\n",
      "90.2419612586247\n",
      "90.8686415451429\n",
      "91.49532183166113\n",
      "92.12200211817937\n",
      "92.74868240469759\n",
      "93.37536269121583\n",
      "94.00204297773405\n",
      "94.62872326425227\n",
      "95.2554035507705\n",
      "95.88208383728873\n",
      "96.50876412380696\n",
      "97.13544441032519\n",
      "97.76212469684341\n",
      "98.38880498336164\n",
      "99.01548526987986\n",
      "99.64216555639808\n"
     ]
    }
   ],
   "source": [
    "def data_from_comment(comment):\n",
    "    vec = []\n",
    "    clean = [ x.lower() for x in word_tokenize(comment)]\n",
    "    for sel in selected:\n",
    "        if sel in clean:\n",
    "            vec.append(model[sel])\n",
    "        else:\n",
    "            vec.append(np.zeros(100))\n",
    "    return vec\n",
    "    \n",
    "        \n",
    "        \n",
    "a_data = [] \n",
    "a_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    if index %1000 ==0:\n",
    "        print(index/len(df)*100)\n",
    "    included = False\n",
    "    temp_data = data_from_comment(row[\"comment_text\"])\n",
    "    for counter, include in enumerate(row[LABELS].values):\n",
    "        if include == 1:\n",
    "            one_hot = np.zeros(len(LABELS))\n",
    "            one_hot[counter] = 1\n",
    "            a_labels.append(one_hot)\n",
    "            a_data.append(temp_data)\n",
    "            included = True\n",
    "            \n",
    "    if not included:\n",
    "        a_labels.append(np.zeros(len(LABELS)))\n",
    "        a_data.append(temp_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(a_data)\n",
    "Y = np.array(a_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_imports\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Save an array to a binary file in NumPy ``.npy`` format.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "file : file, str, or pathlib.Path\n",
       "    File or filename to which the data is saved.  If file is a file-object,\n",
       "    then the filename is unchanged.  If file is a string or Path, a ``.npy``\n",
       "    extension will be appended to the file name if it does not already\n",
       "    have one.\n",
       "arr : array_like\n",
       "    Array data to be saved.\n",
       "allow_pickle : bool, optional\n",
       "    Allow saving object arrays using Python pickles. Reasons for disallowing\n",
       "    pickles include security (loading pickled data can execute arbitrary\n",
       "    code) and portability (pickled objects may not be loadable on different\n",
       "    Python installations, for example if the stored objects require libraries\n",
       "    that are not available, and not all pickled data is compatible between\n",
       "    Python 2 and Python 3).\n",
       "    Default: True\n",
       "fix_imports : bool, optional\n",
       "    Only useful in forcing objects in object arrays on Python 3 to be\n",
       "    pickled in a Python 2 compatible way. If `fix_imports` is True, pickle\n",
       "    will try to map the new Python 3 names to the old module names used in\n",
       "    Python 2, so that the pickle data stream is readable with Python 2.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "savez : Save several arrays into a ``.npz`` archive\n",
       "savetxt, load\n",
       "\n",
       "Notes\n",
       "-----\n",
       "For a description of the ``.npy`` format, see :py:mod:`numpy.lib.format`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from tempfile import TemporaryFile\n",
       ">>> outfile = TemporaryFile()\n",
       "\n",
       ">>> x = np.arange(10)\n",
       ">>> np.save(outfile, x)\n",
       "\n",
       ">>> outfile.seek(0) # Only needed here to simulate closing & reopening file\n",
       ">>> np.load(outfile)\n",
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/gen/lib/python3.6/site-packages/numpy/lib/npyio.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X.shape,Y.shape)\n",
    "np.save(os.path.join(data_dir,\"X_ALL.npy\"),X)\n",
    "np.save(os.path.join(data_dir,\"Y_ALL.npy\"),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
